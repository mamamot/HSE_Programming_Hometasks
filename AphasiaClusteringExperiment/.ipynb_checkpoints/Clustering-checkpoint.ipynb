{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация существительных по левому глагольному контексту\n",
    "Наша гипотеза состоит в том, что мы можем с помощью алгоритма кластеризации найти группы глаголов, которые ведут себя сходным образом. Для ее проверки мы исследуем существительные в подходящих контекстах: биграммы с переходными глаголами и существительными в соответствующих падежах (кроме именительного и предложного) и триграммы с переходными глаголами, предлогом и существительным.\n",
    "\n",
    "## Данные\n",
    "Данные взяты из НКРЯ (поиск по биграммам и триграммам).\n",
    "### Запрос:\n",
    "Обращение к данным происходит путем подстановки леммы в заранее сформированный запрос (см. reference.txt), извлечение данных - с помощью XPath.\n",
    "\n",
    "### Существительные:\n",
    "В качестве леммы в запросе к корпусу использовались существительные, собранные нами из пособий, а также те слова, которые были получены в результате прошлого интерактива.\n",
    "\n",
    "### Выход:\n",
    "Мы получаем данные в виде html-страницы с таблицей, строки которой имеют вид:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Формат строки в таблице (биграммы):\n",
    "<tr><td align=\"right\">27</td><td align=\"right\">15</td><td align=\"right\">13</td><td width=\"100%\"> <span class=\"b-wrd-expl g-em\" explain=\"%D0%BB%D1%8E%D0%B1%D0%B8%D0%BB%D0%B8%7C134%7C10%7C0%7C0%7C0%7C0%7C0\">любили</span> <span class=\"b-wrd-expl g-em\" explain=\"%D0%BE%D1%82%D1%86%D0%B0%7C1087%7C1%7C0%7C0%7C0%7C0%7C0\">отца</span> </td></tr>\n",
    "Формат строки в таблице (триграммы): \n",
    "<tr><td align=\"right\">4</td><td align=\"right\">45</td><td align=\"right\">36</td><td width=\"100%\"> <span class=\"b-wrd-expl g-em\" explain=\"%D0%B2%D0%B7%D0%B3%D0%BB%D1%8F%D0%BD%D1%83%D0%BB%7C377%7C60%7C0%7C0%7C0%7C0%7C0\">взглянул</span> <span class=\"b-wrd-expl g-em\" explain=\"%D0%BD%D0%B0%7C18%7C1%7C0%7C0%7C0%7C0%7C0\">на</span> <span class=\"b-wrd-expl g-em\" explain=\"%D0%BE%D1%82%D1%86%D0%B0%7C691%7C10%7C0%7C0%7C0%7C0%7C0\">отца</span> </td></tr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import pymorphy2\n",
    "import json\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "BIGRAM_LINK = \"http://search.ruscorpora.ru/search.xml?env=sas1_2&mycorp=&mysent=&mysize=&mysentsize=\" \\\n",
    "              \"&dpp=100&spp=100&spd=100&text=lexgramm&mode=ngrams_2_lexgr&sort=gr_freq&lang=ru&nodia=1\" \\\n",
    "              \"&parent1=0&level1=0&lex1=&gramm1=V%2Ctran&flags1=&parent2=0&level2=0&min2=1&max2=1&lex2={0}\" \\\n",
    "              \"&gramm2=%28gen%7Cgen2%7Cdat%7Cacc%7Cacc2%7Cins%29&flags2=\"\n",
    "TRIGRAM_LINK = \"http://search.ruscorpora.ru/search.xml?env=sas1_2&mycorp=&mysent=&mysize=&mysentsize=\" \\\n",
    "               \"&dpp=100&spp=100&spd=100&text=lexgramm&mode=ngrams_3_lexgr&sort=gr_freq&lang=ru&nodia=1\" \\\n",
    "               \"&parent1=0&level1=0&lex1=&gramm1=V&flags1=&parent2=0&level2=0&min2=1&max2=1&lex2=&gramm2=\" \\\n",
    "               \"PR&flags2=&parent3=0&level3=0&min3=1&max3=1&lex3={0}&gramm3=&flags3=\"\n",
    "\n",
    "agent_name = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "            \n",
    "            \n",
    "# загрузка страниц\n",
    "def load_page(url, encoding=\"cp1251\"):\n",
    "    \"\"\"\n",
    "    Простая функция для загрузки страницы с сайта\n",
    "\n",
    "    :param url: URI of the page\n",
    "    :param encoding: page encoding\n",
    "    :return: tuple: a boolean showing success, content of the page (or error message), and http code if available (or 0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = urllib.request.Request(urllib.parse.quote(url, safe=\":/&=?%\"), headers={'User-Agent': agent_name})\n",
    "        with urllib.request.urlopen(req) as r:\n",
    "            code = r.getcode()\n",
    "            page = r.read().decode(encoding)\n",
    "            loaded = True\n",
    "    except urllib.error.HTTPError as e:\n",
    "        page = e.reason\n",
    "        code = e.code\n",
    "        loaded = False\n",
    "    except urllib.error.URLError as e:\n",
    "        page = e.reason\n",
    "        code = 0\n",
    "        loaded = False\n",
    "    except Exception as e:\n",
    "        page = str(e)\n",
    "        code = 0\n",
    "        loaded = False\n",
    "    return loaded, page, code\n",
    "\n",
    "def load_words():\n",
    "    \"\"\"\n",
    "    Загрузка списка слов\n",
    "    \"\"\"\n",
    "    with open(\"w1.txt\", \"r\") as f:\n",
    "        txt = f.read()\n",
    "        words1 = txt.split(\"\\n\")\n",
    "        words1 = [w.strip().lower() for w in words1]\n",
    "    with open(\"w2.txt\", \"r\") as f:\n",
    "        txt = f.read()\n",
    "        words2 = txt.split(\"\\n\")\n",
    "        words2 = [w.strip().lower() for w in words2]\n",
    "    words = set(words1 + words2)\n",
    "    return words\n",
    "\n",
    "def create_link(word, is_trigram=False):\n",
    "    \"\"\"\n",
    "    Создает ссылки на страницы в соответствии с необходимым запросом\n",
    "    \"\"\"\n",
    "    if is_trigram:\n",
    "        return TRIGRAM_LINK.format(word)\n",
    "    else:\n",
    "        return BIGRAM_LINK.format(word)\n",
    "\n",
    "def lemma(token, morph):\n",
    "    parse = morph.parse(token)\n",
    "    for p in parse:\n",
    "        if 'VERB' in p.tag:\n",
    "            return True, p.normal_form\n",
    "    return False, \"\"\n",
    "\n",
    "def extract_words_freqs(bi_page, tri_page):\n",
    "    \"\"\"\n",
    "    Извлекает из текста страницы глаголы/глаголы с предлогами и возвращает в виде словаря, где глаголы - ключи, частоты - \n",
    "    значения.\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "    bi_tree = lxml.html.fromstring(bi_page)\n",
    "    # обходим таблицу на странице по строкам\n",
    "    for tr in bi_tree.iter('tr'):\n",
    "        # прямо в ячейках живут цифры (номер пп. и частотность)\n",
    "        td_text = tr.xpath(\".//td/text()\")\n",
    "        if not td_text:\n",
    "            continue\n",
    "        # в тегах span живут слова\n",
    "        span_text = tr.xpath(\".//td/span/text()\")\n",
    "        freq = int(td_text[1])\n",
    "        token = span_text[0]\n",
    "        is_verb, word = lemma(token, morph)\n",
    "        if is_verb:\n",
    "            if word in list(output_dict.keys()):\n",
    "                output_dict[word] = output_dict[word] + freq\n",
    "            else:\n",
    "                output_dict[word] = freq\n",
    "    tri_tree = lxml.html.fromstring(tri_page)\n",
    "    # переходим к триграммам\n",
    "    for tr in tri_tree.iter('tr'):\n",
    "        td_text = tr.xpath(\".//td/text()\")\n",
    "        if not td_text:\n",
    "            continue\n",
    "        # в тегах span живут слова\n",
    "        span_text = tr.xpath(\".//td/span/text()\")\n",
    "        freq = int(td_text[1])\n",
    "        token = span_text[0]\n",
    "        is_verb, word = lemma(token, morph)\n",
    "        prep_phrase = word + \" \" + span_text[1]\n",
    "        if is_verb:\n",
    "            if prep_phrase in list(output_dict.keys()):\n",
    "                output_dict[prep_phrase] = output_dict[prep_phrase] + freq\n",
    "            else:\n",
    "                output_dict[prep_phrase] = freq\n",
    "    return output_dict\n",
    "\n",
    "def loader(words):\n",
    "    output_dict = dict()\n",
    "    counter = 1\n",
    "    for word in words:\n",
    "        print(\"Dumping data for {0}.\".format(word))\n",
    "        bi_loaded, bi_page, code = load_page(create_link(word, False))\n",
    "        tri_loaded, tri_page, code = load_page(create_link(word, True))\n",
    "        if bi_loaded and tri_loaded:\n",
    "            output_dict[word] = extract_words_freqs(bi_page, tri_page)\n",
    "            print(\"[{0}] Completed.\".format(str(counter)))\n",
    "            counter += 1\n",
    "    return output_dict\n",
    "\n",
    "def dumper():\n",
    "    print(\"Starting the job\")\n",
    "    words = list(load_words())\n",
    "    print(\"Wordlist is loaded: {0} entries\".format(str(len(words))))\n",
    "    loaded = loader(words)\n",
    "    with open(\"out.json\", \"w\") as w:\n",
    "        json.dump(loaded, w)\n",
    "    return loaded\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
